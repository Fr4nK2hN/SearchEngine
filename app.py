from flask import Flask, request, jsonify, render_template
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
import json
import logging
import os
from sentence_transformers import CrossEncoder
from pythonjsonlogger import jsonlogger
import uuid

# å¯¼å…¥ LTR ç›¸å…³æ¨¡å—
from ranking.feature_extractor import FeatureExtractor
from ranking.ranker import LTRRanker

app = Flask(__name__)
es = Elasticsearch([{'host': 'elasticsearch', 'port': 9200, 'scheme': 'http'}])

# åŠ è½½ Cross-Encoder æ¨¡å‹
cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

# ========== åˆå§‹åŒ– LTR ç»„ä»¶ ==========
print("Initializing LTR components...")
feature_extractor = FeatureExtractor()
ltr_ranker = LTRRanker(feature_extractor)

# åŠ è½½è®­ç»ƒå¥½çš„ LTR æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
LTR_MODEL_PATH = 'models/ltr_model.pkl'
if os.path.exists(LTR_MODEL_PATH):
    try:
        ltr_ranker.load_model(LTR_MODEL_PATH)
        print(f"âœ“ LTR model loaded from {LTR_MODEL_PATH}")
    except Exception as e:
        print(f"âœ— Failed to load LTR model: {e}")
        ltr_ranker = None
else:
    print(f"âœ— LTR model not found at {LTR_MODEL_PATH}")
    ltr_ranker = None

# ========== Structured Logging Setup ==========
# Configure structured logging
log_dir = 'logs'
log_file = os.path.join(log_dir, 'events.log')
os.makedirs(log_dir, exist_ok=True)  # Ensure the log directory exists

log_handler = logging.FileHandler(log_file)
formatter = jsonlogger.JsonFormatter('%(asctime)s %(name)s %(levelname)s %(message)s')
log_handler.setFormatter(formatter)
logger = logging.getLogger('search_app')
logger.addHandler(log_handler)
logger.setLevel(logging.INFO)


def create_index_and_bulk_index():
    """åˆ›å»ºç´¢å¼•å¹¶å¯¼å…¥å¤„ç†åçš„æ•°æ®"""
    index_name = "documents"
    
    if not es.indices.exists(index=index_name):
        print(f"ç´¢å¼• '{index_name}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
        
        mappings = {
            "properties": {
                "title": {
                    "type": "text",
                    "analyzer": "standard",
                    "fields": {"keyword": {"type": "keyword"}}
                },
                "content": {
                    "type": "text",
                    "analyzer": "standard"
                },
                "content_full": {
                    "type": "text",
                    "analyzer": "standard"
                },
                "keywords": {
                    "type": "keyword"
                },
                "related_queries": {
                    "type": "text",
                    "analyzer": "standard"
                },
                "combined_text": {
                    "type": "text",
                    "analyzer": "standard"
                },
                "quality": {
                    "type": "keyword"
                }
            }
        }
        
        es.indices.create(index=index_name, mappings=mappings)
        print(f"ç´¢å¼• '{index_name}' åˆ›å»ºå®Œæˆ")

        # å°è¯•åŠ è½½å¤„ç†åçš„æ•°æ®ï¼ˆä¼˜å…ˆ 100k æ–‡ä»¶åï¼‰ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨åŸå§‹æ•°æ®
        data_paths_processed = [
            "data/msmarco_100k_processed.json",
            "data/msmarco_docs_processed.json"
        ]
        data_paths_raw = [
            "data/msmarco_100k.json",
            "data/msmarco_docs.json"
        ]

        documents = None
        for p in data_paths_processed:
            try:
                with open(p, "r", encoding="utf-8") as f:
                    documents = json.load(f)
                print(f"âœ“ ä½¿ç”¨å¤„ç†åçš„æ•°æ®: {p}")
                break
            except FileNotFoundError:
                continue

        if documents is None:
            print("âš  æœªæ‰¾åˆ°å¤„ç†åçš„æ•°æ®ï¼Œå°è¯•ä½¿ç”¨åŸå§‹æ•°æ®")
            for p in data_paths_raw:
                try:
                    with open(p, "r", encoding="utf-8") as f:
                        documents = json.load(f)
                    print(f"âœ“ ä½¿ç”¨åŸå§‹æ•°æ®: {p}")
                    break
                except FileNotFoundError:
                    continue
            if documents is None:
                raise FileNotFoundError("æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆç”Ÿæˆæˆ–ä¸‹è½½æ•°æ®é›†")

        actions = []
        for doc in documents:
            action = {
                "_index": index_name,
                "_id": doc.get("id", ""),
                "_source": {
                    "title": doc.get("title", ""),
                    "content": doc.get("content", ""),
                    "content_full": doc.get("content_full", doc.get("content", "")),
                    "keywords": doc.get("keywords", []),
                    "related_queries": doc.get("related_queries", []),
                    "combined_text": doc.get("combined_text", ""),
                    "quality": doc.get("quality", "unknown")
                }
            }
            actions.append(action)

        # ç«‹å³åˆ·æ–°ç´¢å¼•ï¼Œé¿å…é¦–æ¬¡æœç´¢ç©ºç»“æœ
        bulk(es, actions, refresh=True)
        print(f"âœ“ æ‰¹é‡ç´¢å¼•å®Œæˆï¼Œå…± {len(actions)} ä¸ªæ–‡æ¡£")
    else:
        print(f"ç´¢å¼• '{index_name}' å·²å­˜åœ¨")


@app.route('/')
def index():
    return render_template('index.html')


@app.route('/search')
def search():
    query = request.args.get('q', '')
    mode = request.args.get('mode', 'ltr')  # é»˜è®¤ä½¿ç”¨ LTR
    search_id = str(uuid.uuid4())

    log_entry = {
        "type": "query_submitted",
        "searchId": search_id,
        "query": query,
        "mode": mode,
    }
    
    if not query:
        logger.info("Empty query received", extra=log_entry)
        return jsonify({"results": [], "search_id": search_id})
    
    try:
        # ========== Stage 1: Elasticsearch å¬å› ==========
        # å¯¹çŸ­å•è¯ç¦ç”¨æ¨¡ç³ŠåŒ¹é…ï¼Œé¿å… "java" å‘½ä¸­ "lava" ç­‰é”™è¯¯å¬å›
        tokens = query.strip().split()
        is_short_single_term = len(tokens) == 1 and len(tokens[0]) <= 4

        multi_match = {
            "query": query,
            "fields": ["title^3", "content^2", "combined_text^1.5"],
            "type": "best_fields"
        }
        if not is_short_single_term:
            multi_match["fuzziness"] = "AUTO"

        es_query = {
            "query": {
                "bool": {
                    "should": [
                        {"multi_match": multi_match},
                        {
                            "match_phrase": {
                                "content": {
                                    "query": query,
                                    "boost": 2
                                }
                            }
                        },
                        {
                            "match": {
                                "related_queries": {
                                    "query": query,
                                    "boost": 2
                                }
                            }
                        }
                    ],
                    "minimum_should_match": 1
                }
            },
            "highlight": {
                "fields": {
                    "title": {},
                    "content": {}
                }
            },
            "size": 100  # å¬å›æ›´å¤šå€™é€‰
        }
        
        response = es.search(index="documents", body=es_query)
        results = response['hits']['hits']
        
        if not results:
            log_entry['results_count'] = 0
            logger.info("No results found", extra=log_entry)
            return jsonify({"results": [], "search_id": search_id})
        
        # ========== Stage 2: é€‰æ‹©æ’åºç­–ç•¥ ==========
        
        if mode == 'ltr' and ltr_ranker and ltr_ranker.is_trained:
            # ä½¿ç”¨ LTR æ¨¡å‹é‡æ’åº
            results = ltr_ranker.rerank(query, results)
            ranking_method = 'LTR'
            
        elif mode == 'cross_encoder':
            # ä½¿ç”¨ Cross-Encoder é‡æ’åº
            results = cross_encoder_rerank(query, results)
            ranking_method = 'Cross-Encoder'
            
        elif mode == 'hybrid':
            # æ··åˆæ–¹æ³•ï¼šLTR + Cross-Encoder
            if ltr_ranker and ltr_ranker.is_trained:
                results = hybrid_rerank(query, results)
                ranking_method = 'Hybrid (LTR + Cross-Encoder)'
            else:
                results = cross_encoder_rerank(query, results)
                ranking_method = 'Cross-Encoder (LTR unavailable)'
                
        elif mode == 'baseline':
            # ä»…ä½¿ç”¨ Elasticsearch åˆ†æ•°
            ranking_method = 'Baseline (ES only)'
            
        else:
            # é»˜è®¤ï¼šå¦‚æœæœ‰ LTR å°±ç”¨ LTRï¼Œå¦åˆ™ç”¨ Cross-Encoder
            if ltr_ranker and ltr_ranker.is_trained:
                results = ltr_ranker.rerank(query, results)
                ranking_method = 'LTR'
            else:
                results = cross_encoder_rerank(query, results)
                ranking_method = 'Cross-Encoder'
        
        # ========== Stage 3: è¿”å›ç»“æœ ==========
        final_results = results[:10]
        
        # æ·»åŠ æ’åºæ–¹æ³•ä¿¡æ¯ï¼ˆç”¨äºåˆ†æï¼‰
        for result in final_results:
            result['_ranking_method'] = ranking_method

        log_entry.update({
            "rankingMethod": ranking_method,
            "results_count": len(final_results),
            "result_ids": [r['_id'] for r in final_results]
        })
        logger.info("Search successful", extra=log_entry)
        
        return jsonify({"results": final_results, "search_id": search_id})
        
    except Exception as e:
        import traceback
        log_entry['error'] = str(e)
        log_entry['traceback'] = traceback.format_exc()
        logger.error("Search failed", extra=log_entry)
        return jsonify({"error": "Search failed", "details": str(e), "search_id": search_id}), 500


def cross_encoder_rerank(query, results):
    """ä½¿ç”¨ Cross-Encoder é‡æ’åº"""
    passages = [hit['_source']['content'] for hit in results]
    pairs = [[query, passage] for passage in passages]
    
    cross_scores = cross_encoder_model.predict(pairs)
    
    for i, hit in enumerate(results):
        original_score = hit['_score']
        cross_score = float(cross_scores[i])
        # æ··åˆåˆ†æ•°
        hit['_score'] = 0.3 * original_score + 0.7 * cross_score
        hit['_cross_score'] = cross_score
    
    results.sort(key=lambda x: x['_score'], reverse=True)
    return results


def hybrid_rerank(query, results):
    """æ··åˆé‡æ’åºï¼šLTR + Cross-Encoder"""
    # å…ˆç”¨ LTR é‡æ’
    results = ltr_ranker.rerank(query, results)
    
    # å†ç”¨ Cross-Encoder å¾®è°ƒ top 20
    top_results = results[:20]
    passages = [hit['_source']['content'] for hit in top_results]
    pairs = [[query, passage] for passage in passages]
    
    cross_scores = cross_encoder_model.predict(pairs)
    
    for i, hit in enumerate(top_results):
        ltr_score = hit.get('_ltr_score', hit['_score'])
        cross_score = float(cross_scores[i])
        # æ··åˆ LTR å’Œ Cross-Encoder åˆ†æ•°
        hit['_score'] = 0.6 * ltr_score + 0.4 * cross_score
        hit['_hybrid_components'] = {
            'ltr': ltr_score,
            'cross': cross_score
        }
    
    top_results.sort(key=lambda x: x['_score'], reverse=True)
    
    # åˆå¹¶å›å®Œæ•´ç»“æœ
    results[:20] = top_results
    return results


@app.route('/log', methods=['POST'])
def log_event():
    events = request.get_json()
    if not events:
        return jsonify({"status": "error", "message": "No data provided"}), 400

    for event in events:
        logger.info(json.dumps(event))

    return jsonify({"status": "success"}), 200


@app.route('/export_data')
def export_data():
    """å¯¼å‡ºç”¨æˆ·äº¤äº’æ•°æ®"""
    try:
        from datetime import datetime
        events_data = []
        if os.path.exists('logs/events.log'):
            with open('logs/events.log', 'r') as f:
                for line in f:
                    try:
                        # æ¯è¡Œå¯èƒ½æ˜¯å•ä¸ªäº‹ä»¶æˆ–äº‹ä»¶æ•°ç»„
                        line_data = json.loads(line.strip().split(' ', 1)[1])  # è·³è¿‡æ—¶é—´æˆ³
                        if isinstance(line_data, list):
                            events_data.extend(line_data)
                        else:
                            events_data.append(line_data)
                    except (json.JSONDecodeError, IndexError):
                        continue
        
        summary = {
            'total_sessions': len(set(event.get('sessionId', '') for event in events_data)),
            'total_events': len(events_data),
            'event_types': {},
            'query_stats': {
                'total_queries': 0,
                'unique_queries': set(),
                'abandoned_queries': 0
            },
            'interaction_stats': {
                'total_clicks': 0,
                'total_scrolls': 0,
                'average_session_duration': 0
            }
        }
        
        session_durations = []
        
        for event in events_data:
            event_type = event.get('type', 'unknown')
            summary['event_types'][event_type] = summary['event_types'].get(event_type, 0) + 1
            
            if event_type == 'query_submitted':
                summary['query_stats']['total_queries'] += 1
                summary['query_stats']['unique_queries'].add(event.get('query', ''))
            elif event_type == 'query_abandoned':
                summary['query_stats']['abandoned_queries'] += 1
            elif event_type == 'result_clicked':
                summary['interaction_stats']['total_clicks'] += 1
            elif event_type == 'scroll_action':
                summary['interaction_stats']['total_scrolls'] += 1
            elif event_type == 'session_end':
                duration = event.get('totalDuration', 0)
                if duration > 0:
                    session_durations.append(duration)
        
        summary['query_stats']['unique_queries'] = len(summary['query_stats']['unique_queries'])
        if session_durations:
            summary['interaction_stats']['average_session_duration'] = sum(session_durations) / len(session_durations)
        
        export_data = {
            'export_timestamp': datetime.now().isoformat(),
            'summary': summary,
            'raw_events': events_data
        }
        
        return jsonify(export_data)
        
    except Exception as e:
        return jsonify({"error": f"Export failed: {str(e)}"}), 500


@app.route('/research_dashboard')
def research_dashboard():
    """ç ”ç©¶ä»ªè¡¨ç›˜"""
    try:
        events_data = []
        if os.path.exists('logs/events.log'):
            with open('logs/events.log', 'r') as f:
                for line in f.readlines()[-100:]:
                    try:
                        line_data = json.loads(line.strip().split(' ', 1)[1])
                        if isinstance(line_data, list):
                            events_data.extend(line_data)
                        else:
                            events_data.append(line_data)
                    except (json.JSONDecodeError, IndexError):
                        continue
        
        sessions = {}
        for event in events_data:
            session_id = event.get('sessionId', 'unknown')
            if session_id not in sessions:
                sessions[session_id] = []
            sessions[session_id].append(event)
        
        dashboard_html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Research Dashboard - LTR Enhanced</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }}
                .container {{ max-width: 1200px; margin: 0 auto; }}
                .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                          color: white; padding: 30px; border-radius: 10px; margin-bottom: 20px; }}
                .summary {{ background: white; padding: 20px; margin-bottom: 20px; 
                           border-radius: 10px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
                .stats-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); 
                              gap: 15px; margin-top: 15px; }}
                .stat-card {{ background: #f8f9fa; padding: 15px; border-radius: 8px; 
                             border-left: 4px solid #667eea; }}
                .stat-value {{ font-size: 28px; font-weight: bold; color: #667eea; }}
                .stat-label {{ color: #666; font-size: 14px; margin-top: 5px; }}
                .session {{ border: 1px solid #ddd; margin: 10px 0; padding: 15px; 
                           background: white; border-radius: 8px; }}
                .event {{ margin: 8px 0; padding: 10px; background: #f8f9fa; 
                         border-radius: 5px; font-size: 14px; }}
                .event-type {{ display: inline-block; padding: 3px 8px; background: #667eea; 
                              color: white; border-radius: 3px; font-size: 12px; font-weight: bold; }}
                h1, h2 {{ margin: 0; }}
                .ltr-badge {{ background: #28a745; color: white; padding: 5px 10px; 
                             border-radius: 5px; font-size: 14px; display: inline-block; }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>ğŸ”¬ Search Engine Research Dashboard</h1>
                    <p style="margin-top: 10px; opacity: 0.9;">
                        <span class="ltr-badge">LTR Enabled</span> 
                        Real-time monitoring of search behavior and ranking performance
                    </p>
                </div>
                
                <div class="summary">
                    <h2>ğŸ“Š System Statistics</h2>
                    <div class="stats-grid">
                        <div class="stat-card">
                            <div class="stat-value">{len(sessions)}</div>
                            <div class="stat-label">Total Sessions</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-value">{len(events_data)}</div>
                            <div class="stat-label">Total Events</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-value">{'âœ“' if ltr_ranker and ltr_ranker.is_trained else 'âœ—'}</div>
                            <div class="stat-label">LTR Model Status</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-value">{len(feature_extractor.get_feature_names())}</div>
                            <div class="stat-label">Feature Dimensions</div>
                        </div>
                    </div>
                </div>
                
                <div class="summary">
                    <h2>ğŸ”¥ Recent Sessions</h2>
        """
        
        for session_id, session_events in list(sessions.items())[:10]:
            dashboard_html += f"""
            <div class="session">
                <h3>Session: {session_id[:16]}...</h3>
                <p style="color: #666;">Events: {len(session_events)}</p>
            """
            
            for event in session_events[-5:]:
                event_type = event.get('type', 'unknown')
                timestamp = event.get('timestamp', 'no timestamp')
                query = event.get('query', '')
                ranking_method = event.get('rankingMethod', 'N/A')
                
                dashboard_html += f"""
                <div class="event">
                    <span class="event-type">{event_type}</span>
                    <span style="color: #999; margin-left: 10px;">{timestamp}</span>
                    {f'<br><strong>Query:</strong> {query}' if query else ''}
                    {f'<br><strong>Ranking:</strong> {ranking_method}' if ranking_method != 'N/A' else ''}
                </div>
                """
            
            dashboard_html += "</div>"
        
        dashboard_html += """
                </div>
            </div>
        </body>
        </html>
        """
        
        return dashboard_html
        
    except Exception as e:
        return f"<html><body><h1>Error</h1><p>{str(e)}</p></body></html>"


# ========== æ–°å¢ï¼šè®­ç»ƒ LTR æ¨¡å‹çš„ç«¯ç‚¹ ==========

@app.route('/train_ltr', methods=['POST'])
def train_ltr_endpoint():
    """
    è®­ç»ƒæˆ–é‡æ–°è®­ç»ƒ LTR æ¨¡å‹
    
    POST body:
    {
        "num_queries": 50,
        "n_estimators": 200,
        "learning_rate": 0.05
    }
    """
    try:
        params = request.get_json() or {}
        num_queries = params.get('num_queries', 50)
        n_estimators = params.get('n_estimators', 200)
        learning_rate = params.get('learning_rate', 0.05)
        
        from ranking.training_data_generator import TrainingDataGenerator
        
        # ç”Ÿæˆè®­ç»ƒæ•°æ®
        generator = TrainingDataGenerator(es, cross_encoder_model)
        queries = generator.generate_training_queries(num_queries=num_queries)
        training_data = generator.generate_training_data(queries, docs_per_query=30)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        split_idx = int(len(training_data) * 0.8)
        train_set = training_data[:split_idx]
        val_set = training_data[split_idx:]
        
        # è®­ç»ƒæ¨¡å‹
        global ltr_ranker
        ltr_ranker = LTRRanker(feature_extractor)
        ltr_ranker.train(
            training_data=train_set,
            validation_data=val_set,
            n_estimators=n_estimators,
            learning_rate=learning_rate
        )
        
        # ä¿å­˜æ¨¡å‹
        os.makedirs('models', exist_ok=True)
        ltr_ranker.save_model(LTR_MODEL_PATH)
        
        return jsonify({
            "status": "success",
            "message": "LTR model trained successfully",
            "training_queries": len(train_set),
            "validation_queries": len(val_set)
        })
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500


@app.route('/model_info')
def model_info():
    """è·å– LTR æ¨¡å‹ä¿¡æ¯"""
    if not ltr_ranker or not ltr_ranker.is_trained:
        return jsonify({
            "status": "not_trained",
            "message": "LTR model not available"
        })
    
    # è·å–ç‰¹å¾é‡è¦æ€§
    importances = ltr_ranker.model.feature_importances_
    feature_importance = {
        name: float(importance)
        for name, importance in zip(ltr_ranker.feature_names, importances)
    }
    
    # æ’åº
    sorted_features = sorted(
        feature_importance.items(),
        key=lambda x: x[1],
        reverse=True
    )[:20]
    
    return jsonify({
        "status": "trained",
        "n_features": len(ltr_ranker.feature_names),
        "feature_names": ltr_ranker.feature_names,
        "top_features": dict(sorted_features),
        "model_params": {
            "n_estimators": ltr_ranker.model.n_estimators,
            "max_depth": ltr_ranker.model.max_depth,
            "learning_rate": ltr_ranker.model.learning_rate
        }
    })


@app.route('/track_click', methods=['POST'])
def track_click():
    data = request.get_json()
    log_entry = {
        "event": "click",
        "search_id": data.get("search_id"),
        "doc_id": data.get("doc_id"),
        "rank": data.get("rank"),
        "query": data.get("query"),
    }
    logger.info("Document clicked", extra=log_entry)
    return jsonify({"status": "success"}), 200


if __name__ == '__main__':
    create_index_and_bulk_index()
    app.run(host='0.0.0.0', port=5000, debug=True)