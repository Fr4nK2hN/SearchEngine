import os
import sys
import argparse
import json

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

print("æ­£åœ¨å¯¼å…¥ä¾èµ–...")

try:
    from elasticsearch import Elasticsearch
    print("âœ“ Elasticsearch å·²å¯¼å…¥")
except ImportError:
    print("âœ— è¯·å®‰è£…: pip install elasticsearch")
    sys.exit(1)

try:
    from sentence_transformers import CrossEncoder
    print("âœ“ SentenceTransformers å·²å¯¼å…¥")
except ImportError:
    print("âœ— è¯·å®‰è£…: pip install sentence-transformers")
    sys.exit(1)

try:
    import lightgbm
    print("âœ“ LightGBM å·²å¯¼å…¥")
except ImportError:
    print("âœ— è¯·å®‰è£…: pip install lightgbm")
    sys.exit(1)

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
try:
    from ranking.feature_extractor import FeatureExtractor
    from ranking.training_data_generator import TrainingDataGenerator
    from ranking.ranker import LTRRanker
    from ranking.evaluator import RankingEvaluator
    print("âœ“ è‡ªå®šä¹‰æ¨¡å—å·²å¯¼å…¥")
except ImportError as e:
    print(f"âœ— å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—å¤±è´¥: {e}")
    print("\nè¯·ç¡®ä¿:")
    print("1. ranking/ ç›®å½•ä¸‹æœ‰ __init__.py æ–‡ä»¶")
    print("2. æ‰€æœ‰ Python æ–‡ä»¶éƒ½åœ¨ ranking/ ç›®å½•ä¸­")
    sys.exit(1)


def main(args):
    """ä¸»è®­ç»ƒæµç¨‹"""
    
    print("\n" + "="*70)
    print("ğŸš€ Learning to Rank Model Training Pipeline")
    print("="*70)
    
    # ========== 1. åˆå§‹åŒ–ç»„ä»¶ ==========
    print("\n[Step 1/6] åˆå§‹åŒ–ç»„ä»¶...")
    
    es = Elasticsearch([{
        'host': args.es_host,
        'port': args.es_port,
        'scheme': 'http'
    }])
    
    # æµ‹è¯•è¿æ¥
    try:
        if not es.ping():
            print("âœ— æ— æ³•è¿æ¥åˆ° Elasticsearch")
            print(f"  è¯·ç¡®ä¿ Elasticsearch è¿è¡Œåœ¨ {args.es_host}:{args.es_port}")
            sys.exit(1)
        print(f"âœ“ å·²è¿æ¥åˆ° Elasticsearch ({args.es_host}:{args.es_port})")
    except Exception as e:
        print(f"âœ— Elasticsearch è¿æ¥é”™è¯¯: {e}")
        sys.exit(1)
    
    print("æ­£åœ¨åŠ è½½ Cross-Encoder æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œä¼šä¸‹è½½ï¼Œè¯·ç¨å€™ï¼‰...")
    try:
        cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
        print("âœ“ Cross-Encoder æ¨¡å‹å·²åŠ è½½")
    except Exception as e:
        print(f"âœ— åŠ è½½ Cross-Encoder å¤±è´¥: {e}")
        sys.exit(1)
    
    feature_extractor = FeatureExtractor()
    print(f"âœ“ ç‰¹å¾æå–å™¨å·²åˆå§‹åŒ– ({len(feature_extractor.get_feature_names())} ä¸ªç‰¹å¾)")
    
    # ========== 2. ç”Ÿæˆ/åŠ è½½è®­ç»ƒæ•°æ® ==========
    print(f"\n[Step 2/6] å‡†å¤‡è®­ç»ƒæ•°æ®...")
    
    generator = TrainingDataGenerator(es, cross_encoder)
    training_data_path = args.training_data
    
    if os.path.exists(training_data_path) and not args.regenerate_data:
        print(f"ä» {training_data_path} åŠ è½½å·²æœ‰è®­ç»ƒæ•°æ®...")
        try:
            training_data = generator.load_training_data(training_data_path)
        except Exception as e:
            print(f"âœ— åŠ è½½å¤±è´¥: {e}")
            print("å°†é‡æ–°ç”Ÿæˆè®­ç»ƒæ•°æ®...")
            training_data = None
    else:
        training_data = None
    
    if training_data is None:
        print(f"ç”Ÿæˆæ–°çš„è®­ç»ƒæ•°æ®...")
        print(f"  - æŸ¥è¯¢æ•°é‡: {args.num_queries}")
        print(f"  - æ¯ä¸ªæŸ¥è¯¢çš„æ–‡æ¡£æ•°: {args.docs_per_query}")
        
        try:
            # ç”ŸæˆæŸ¥è¯¢
            queries = generator.generate_training_queries(num_queries=args.num_queries)
            print(f"âœ“ ç”Ÿæˆäº† {len(queries)} ä¸ªè®­ç»ƒæŸ¥è¯¢")
            
            # ç”Ÿæˆè®­ç»ƒæ•°æ®
            training_data = generator.generate_training_data(
                queries, 
                docs_per_query=args.docs_per_query
            )
            
            # ä¿å­˜
            os.makedirs(os.path.dirname(training_data_path), exist_ok=True)
            generator.save_training_data(training_data, training_data_path)
            print(f"âœ“ è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ° {training_data_path}")
        except Exception as e:
            print(f"âœ— ç”Ÿæˆè®­ç»ƒæ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    
    # ========== 3. åˆ’åˆ†æ•°æ®é›† ==========
    print(f"\n[Step 3/6] åˆ’åˆ†æ•°æ®é›†...")
    
    # 80% è®­ç»ƒï¼Œ10% éªŒè¯ï¼Œ10% æµ‹è¯•
    n_total = len(training_data)
    n_train = int(n_total * 0.8)
    n_val = int(n_total * 0.1)
    
    train_set = training_data[:n_train]
    val_set = training_data[n_train:n_train + n_val]
    test_set = training_data[n_train + n_val:]
    
    print(f"  - è®­ç»ƒé›†:   {len(train_set)} ä¸ªæŸ¥è¯¢")
    print(f"  - éªŒè¯é›†: {len(val_set)} ä¸ªæŸ¥è¯¢")
    print(f"  - æµ‹è¯•é›†:   {len(test_set)} ä¸ªæŸ¥è¯¢")
    
    if len(train_set) < 5:
        print("âœ— è®­ç»ƒæ•°æ®å¤ªå°‘ï¼è¯·å¢åŠ  --num-queries å‚æ•°")
        sys.exit(1)
    
    # ========== 4. è®­ç»ƒæ¨¡å‹ ==========
    print(f"\n[Step 4/6] è®­ç»ƒ LTR æ¨¡å‹...")
    print(f"  - ç®—æ³•: LambdaMART (LightGBM)")
    print(f"  - n_estimators: {args.n_estimators}")
    print(f"  - learning_rate: {args.learning_rate}")
    print(f"  - max_depth: {args.max_depth}")
    
    try:
        ranker = LTRRanker(feature_extractor)
        ranker.train(
            training_data=train_set,
            validation_data=val_set if len(val_set) > 0 else None,
            n_estimators=args.n_estimators,
            learning_rate=args.learning_rate,
            max_depth=args.max_depth
        )
        print("âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆ")
    except Exception as e:
        print(f"âœ— è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # ========== 5. è¯„ä¼°æ¨¡å‹ ==========
    print(f"\n[Step 5/6] åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
    
    if len(test_set) == 0:
        print("âš  æ²¡æœ‰æµ‹è¯•æ•°æ®ï¼Œè·³è¿‡è¯„ä¼°")
        avg_results = None
    else:
        try:
            evaluator = RankingEvaluator()
            avg_results, full_results = evaluator.evaluate_ranker(
                test_set,
                ranker,
                k_values=[1, 3, 5, 10]
            )
            evaluator.print_results(avg_results)
        except Exception as e:
            print(f"âœ— è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            avg_results = None
    
    # ========== 6. ä¿å­˜æ¨¡å‹å’Œç»“æœ ==========
    print(f"\n[Step 6/6] ä¿å­˜æ¨¡å‹å’Œç›¸å…³æ–‡ä»¶...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    try:
        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(args.output_dir, 'ltr_model.pkl')
        ranker.save_model(model_path)
        print(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")
        
        # ä¿å­˜ç‰¹å¾é‡è¦æ€§å›¾
        try:
            feature_plot_path = os.path.join(args.output_dir, 'feature_importance.png')
            ranker.plot_feature_importance(top_k=20, save_path=feature_plot_path)
        except Exception as e:
            print(f"âš  æ— æ³•ä¿å­˜ç‰¹å¾é‡è¦æ€§å›¾: {e}")
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        if avg_results:
            results_path = os.path.join(args.output_dir, 'evaluation_results.json')
            # è½¬æ¢ numpy ç±»å‹ä¸º Python åŸç”Ÿç±»å‹
            def convert_numpy(obj):
                import numpy as np
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {key: convert_numpy(value) for key, value in obj.items()}
                return obj
            
            avg_results_converted = convert_numpy(avg_results)
            
            with open(results_path, 'w', encoding='utf-8') as f:
                json.dump(avg_results_converted, f, indent=2)
            print(f"âœ“ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ° {results_path}")
    except Exception as e:
        print(f"âœ— ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # ========== é¢å¤–ï¼šå¯¹æ¯”å®éªŒï¼ˆå¯é€‰ï¼‰ ==========
    if args.run_comparison and len(test_set) > 0:
        print(f"\n[Bonus] è¿è¡Œå¯¹æ¯”å®éªŒ...")
        try:
            run_comparison_experiments(
                test_set, 
                ranker, 
                feature_extractor, 
                cross_encoder, 
                args.output_dir
            )
        except Exception as e:
            print(f"âš  å¯¹æ¯”å®éªŒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "="*70)
    print("âœ… è®­ç»ƒæµç¨‹æˆåŠŸå®Œæˆï¼")
    print("="*70)
    print(f"\næ¨¡å‹å’Œç›¸å…³æ–‡ä»¶å·²ä¿å­˜åˆ°: {args.output_dir}/")
    print(f"\nè¦åœ¨ Flask åº”ç”¨ä¸­ä½¿ç”¨æ­¤æ¨¡å‹:")
    print(f"  1. ç¡®ä¿ {model_path} å­˜åœ¨")
    print(f"  2. è¿è¡Œ: python app.py")
    print(f"  3. è®¿é—®: http://localhost:5000/search?q=your_query&mode=ltr")


def run_comparison_experiments(test_set, ltr_ranker, feature_extractor, 
                               cross_encoder, output_dir):
    """è¿è¡Œå¯¹æ¯”å®éªŒï¼šBaseline vs LTR"""
    from ranking.evaluator import RankingEvaluator
    
    print("è®¾ç½®å¯¹æ¯”å®éªŒ...")
    
    # åˆ›å»ºåŸºçº¿æ’åºå™¨
    class BaselineRanker:
        """åŸºçº¿æ’åºå™¨ - åªä½¿ç”¨ Elasticsearch åˆ†æ•°"""
        def predict(self, query, documents):
            return [doc.get('es_score', 0.0) for doc in documents]
    
    class CrossEncoderRanker:
        """Cross-Encoder æ’åºå™¨"""
        def __init__(self, model):
            self.model = model
        
        def predict(self, query, documents):
            passages = [doc['content'][:512] for doc in documents]  # é™åˆ¶é•¿åº¦
            pairs = [[query, p] for p in passages]
            return self.model.predict(pairs)
    
    # å‡†å¤‡æ’åºå™¨
    rankers = {
        'Baseline (ES)': BaselineRanker(),
        'Cross-Encoder': CrossEncoderRanker(cross_encoder),
        'LTR (Ours)': ltr_ranker
    }
    
    # è¯„ä¼°
    evaluator = RankingEvaluator()
    comparison = evaluator.compare_rankers(
        test_set,
        rankers,
        k_values=[1, 3, 5, 10]
    )
    
    # æ‰“å°å¯¹æ¯”ç»“æœ
    print("\n" + "="*70)
    print("å¯¹æ¯”å®éªŒç»“æœ")
    print("="*70)
    
    for ranker_name, results in comparison.items():
        print(f"\n{ranker_name}:")
        print(f"  NDCG@10: {results['ndcg'][10]:.4f}")
        print(f"  MAP:     {results['map']:.4f}")
        print(f"  MRR:     {results['mrr']:.4f}")
    
    # å¯è§†åŒ–å¯¹æ¯”
    try:
        plot_path = os.path.join(output_dir, 'comparison_results.png')
        evaluator.plot_comparison(comparison, save_path=plot_path)
    except Exception as e:
        print(f"âš  æ— æ³•ç”Ÿæˆå¯¹æ¯”å›¾: {e}")
    
    # ä¿å­˜å¯¹æ¯”ç»“æœ
    def convert_numpy(obj):
        import numpy as np
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {key: convert_numpy(value) for key, value in obj.items()}
        return obj
    
    comparison_converted = convert_numpy(comparison)
    comparison_path = os.path.join(output_dir, 'comparison_results.json')
    with open(comparison_path, 'w', encoding='utf-8') as f:
        json.dump(comparison_converted, f, indent=2)
    
    print(f"\nâœ“ å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ° {comparison_path}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='è®­ç»ƒ Learning to Rank æœç´¢æ’åºæ¨¡å‹'
    )
    
    # Elasticsearch é…ç½®
    parser.add_argument('--es-host', type=str, default='localhost',
        help='Elasticsearch ä¸»æœºåœ°å€ (é»˜è®¤: localhost)')
    parser.add_argument('--es-port', type=int, default=9200,
                       help='Elasticsearch ç«¯å£ (é»˜è®¤: 9200)')
    
    # è®­ç»ƒæ•°æ®é…ç½®
    parser.add_argument('--training-data', type=str, 
                       default='data/ltr_training_data.json',
                       help='è®­ç»ƒæ•°æ® JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--regenerate-data', action='store_true',
                       help='å³ä½¿æ–‡ä»¶å­˜åœ¨ä¹Ÿé‡æ–°ç”Ÿæˆè®­ç»ƒæ•°æ®')
    parser.add_argument('--num-queries', type=int, default=50,
                       help='ç”Ÿæˆçš„è®­ç»ƒæŸ¥è¯¢æ•°é‡ (é»˜è®¤: 50)')
    parser.add_argument('--docs-per-query', type=int, default=30,
                       help='æ¯ä¸ªæŸ¥è¯¢çš„æ–‡æ¡£æ•°é‡ (é»˜è®¤: 30)')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--n-estimators', type=int, default=100,
                       help='Boosting è¿­ä»£æ¬¡æ•° (é»˜è®¤: 100)')
    parser.add_argument('--learning-rate', type=float, default=0.05,
                       help='å­¦ä¹ ç‡ (é»˜è®¤: 0.05)')
    parser.add_argument('--max-depth', type=int, default=6,
                       help='æ ‘çš„æœ€å¤§æ·±åº¦ (é»˜è®¤: 6)')
    
    # è¾“å‡ºé…ç½®
    parser.add_argument('--output-dir', type=str, default='models',
                       help='ä¿å­˜æ¨¡å‹çš„ç›®å½• (é»˜è®¤: models)')
    
    # å®éªŒé…ç½®
    parser.add_argument('--run-comparison', action='store_true',
                       help='è¿è¡Œå¯¹æ¯”å®éªŒ (Baseline vs LTR)')
    
    args = parser.parse_args()
    
    main(args)